{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMxOit5hRcHedPYxMqvi51k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuadollison/smallbizpulse/blob/jd-model/notebooks/model_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "FW725wxS3ubd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U2OBThQpHLIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15729d6c-902e-4005-b670-0423441bdd7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSetup complete — libraries loaded, styling configured.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SETUP: Mount Drive, Install Dependencies, Configure Styling\n",
        "# ============================================================\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install VADER for sentiment analysis\n",
        "!pip install vaderSentiment -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ── Consistent Plot Styling ──────────────────────────────────\n",
        "plt.rcParams.update({\n",
        "    'figure.figsize': (12, 6),\n",
        "    'figure.dpi': 120,\n",
        "    'font.family': 'sans-serif',\n",
        "    'font.size': 11,\n",
        "    'axes.titlesize': 14,\n",
        "    'axes.titleweight': 'bold',\n",
        "    'axes.labelsize': 12,\n",
        "    'axes.spines.top': False,\n",
        "    'axes.spines.right': False,\n",
        "    'axes.grid': True,\n",
        "    'grid.alpha': 0.3,\n",
        "    'grid.linestyle': '--',\n",
        "})\n",
        "\n",
        "# SmallBizPulse color palette\n",
        "COLORS = {\n",
        "    'primary': '#2563EB',\n",
        "    'secondary': '#F59E0B',\n",
        "    'open': '#10B981',\n",
        "    'closed': '#EF4444',\n",
        "    'accent1': '#8B5CF6',\n",
        "    'accent2': '#EC4899',\n",
        "    'neutral': '#6B7280',\n",
        "    'bg': '#F9FAFB',\n",
        "}\n",
        "PALETTE_OC = [COLORS['open'], COLORS['closed']]\n",
        "\n",
        "print(\"Setup complete — libraries loaded, styling configured.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# DATA LOADING\n",
        "# ============================================================\n",
        "# >>> UPDATE THIS PATH to match your Google Drive folder <<<\n",
        "DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/CIS509/yelp_dataset_new/'\n",
        "\n",
        "def load_json(filename):\n",
        "    filepath = DATA_PATH + filename\n",
        "    with open(filepath, 'r') as f:\n",
        "        first_char = f.read(1)\n",
        "        f.seek(0)\n",
        "        if first_char == '[':\n",
        "            return pd.DataFrame(json.load(f))\n",
        "        else:\n",
        "            return pd.read_json(f, lines=True)\n",
        "\n",
        "print(\"Loading datasets...\")\n",
        "business_df = load_json('yelp_academic_dataset_business.json')\n",
        "print(f\"  Business: {len(business_df):,} records\")\n",
        "\n",
        "review_df = load_json('yelp_academic_dataset_review.json')\n",
        "print(f\"  Review:   {len(review_df):,} records\")\n",
        "\n",
        "tip_df = load_json('yelp_academic_dataset_tip.json')\n",
        "print(f\"  Tip:      {len(tip_df):,} records\")\n",
        "\n",
        "checkin_df = load_json('yelp_academic_dataset_checkin.json')\n",
        "print(f\"  Checkin:  {len(checkin_df):,} records\")\n",
        "\n",
        "user_df = load_json('yelp_academic_dataset_user.json')\n",
        "print(f\"  User:     {len(user_df):,} records\")\n",
        "\n",
        "print(\"\\nAll datasets loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CbxCviE0qbd",
        "outputId": "bdca4c46-a4f3-4afd-be2f-b9c32d72bb13"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "  Business: 9,973 records\n",
            "  Review:   100,000 records\n",
            "  Tip:      264,693 records\n",
            "  Checkin:  9,337 records\n",
            "  User:     79,345 records\n",
            "\n",
            "All datasets loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# DATA SOURCES & FILTERING CRITERIA\n",
        "# ============================================================\n",
        "\n",
        "# Step 1: Filter for restaurants\n",
        "# A business is classified as a \"restaurant\" if its Yelp categories\n",
        "# contain the word \"Restaurants\" (case-insensitive).\n",
        "restaurant_df = business_df[\n",
        "    business_df['categories'].str.contains('Restaurants', case=False, na=False)\n",
        "].copy()\n",
        "\n",
        "# Step 2: Get restaurant business IDs\n",
        "restaurant_ids = set(restaurant_df['business_id'])\n",
        "\n",
        "# Step 3: Filter reviews to restaurant-only\n",
        "rest_review_df = review_df[review_df['business_id'].isin(restaurant_ids)].copy()\n",
        "rest_review_df['date'] = pd.to_datetime(rest_review_df['date'])\n",
        "rest_review_df['year'] = rest_review_df['date'].dt.year\n",
        "\n",
        "# Step 4: Filter tips to restaurant-only\n",
        "rest_tip_df = tip_df[tip_df['business_id'].isin(restaurant_ids)].copy()\n",
        "\n",
        "# Step 5: Filter checkins to restaurant-only\n",
        "rest_checkin_df = checkin_df[checkin_df['business_id'].isin(restaurant_ids)].copy()\n",
        "\n",
        "# Step 6: Merge business status onto reviews\n",
        "status_map = restaurant_df.set_index('business_id')['is_open'].to_dict()\n",
        "rest_review_df['is_open'] = rest_review_df['business_id'].map(status_map)\n",
        "rest_review_df['status'] = rest_review_df['is_open'].map({1: 'Open', 0: 'Closed'})\n",
        "\n",
        "# ── Print Summary ────────────────────────────────────────────\n",
        "print(\"=\" * 65)\n",
        "print(\"DATA SOURCES & FILTERING SUMMARY\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "print(\"\\nPRIMARY DATA SOURCE: Yelp Academic Dataset\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "print(\"\\nFull Dataset:\")\n",
        "print(f\"  Businesses:  {len(business_df):>8,}\")\n",
        "print(f\"  Reviews:     {len(review_df):>8,}\")\n",
        "print(f\"  Tips:        {len(tip_df):>8,}\")\n",
        "print(f\"  Check-ins:   {len(checkin_df):>8,}\")\n",
        "print(f\"  Users:       {len(user_df):>8,}\")\n",
        "\n",
        "print(\"\\nFiltered to Restaurants (categories contain 'Restaurants'):\")\n",
        "print(f\"  Restaurants:        {len(restaurant_df):>8,}\")\n",
        "print(f\"  Restaurant Reviews: {len(rest_review_df):>8,}\")\n",
        "print(f\"  Restaurant Tips:    {len(rest_tip_df):>8,}\")\n",
        "print(f\"  Restaurant Checkins:{len(rest_checkin_df):>8,}\")\n",
        "\n",
        "n_open = (restaurant_df['is_open'] == 1).sum()\n",
        "n_closed = (restaurant_df['is_open'] == 0).sum()\n",
        "print(\"\\nRestaurant Status:\")\n",
        "print(f\"  Open:   {n_open:>5,}  ({n_open / len(restaurant_df) * 100:.1f}%)\")\n",
        "print(f\"  Closed: {n_closed:>5,}  ({n_closed / len(restaurant_df) * 100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nDate Range: {rest_review_df['date'].min().strftime('%Y-%m-%d')} to \"\n",
        "      f\"{rest_review_df['date'].max().strftime('%Y-%m-%d')}\")\n",
        "\n",
        "print(\"\\nFILTERING CRITERIA APPLIED:\")\n",
        "print(\"  1. Category filter: categories.str.contains('Restaurants')\")\n",
        "print(\"  2. Reviews, tips, and check-ins filtered by restaurant business_id\")\n",
        "print(\"  3. No minimum review count threshold (preserving data-sparse\")\n",
        "print(\"     businesses is important for studying closure patterns)\")\n",
        "print(\"  4. No date range restriction (full temporal span needed for time-series)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYRtI2UO1Izy",
        "outputId": "baf855f9-8221-4548-96aa-7ac1dc8b0bb5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "DATA SOURCES & FILTERING SUMMARY\n",
            "=================================================================\n",
            "\n",
            "PRIMARY DATA SOURCE: Yelp Academic Dataset\n",
            "---------------------------------------------\n",
            "\n",
            "Full Dataset:\n",
            "  Businesses:     9,973\n",
            "  Reviews:      100,000\n",
            "  Tips:         264,693\n",
            "  Check-ins:      9,337\n",
            "  Users:         79,345\n",
            "\n",
            "Filtered to Restaurants (categories contain 'Restaurants'):\n",
            "  Restaurants:           4,132\n",
            "  Restaurant Reviews:   72,124\n",
            "  Restaurant Tips:      20,394\n",
            "  Restaurant Checkins:   4,085\n",
            "\n",
            "Restaurant Status:\n",
            "  Open:   2,575  (62.3%)\n",
            "  Closed: 1,557  (37.7%)\n",
            "\n",
            "Date Range: 2005-03-01 to 2018-10-04\n",
            "\n",
            "FILTERING CRITERIA APPLIED:\n",
            "  1. Category filter: categories.str.contains('Restaurants')\n",
            "  2. Reviews, tips, and check-ins filtered by restaurant business_id\n",
            "  3. No minimum review count threshold (preserving data-sparse\n",
            "     businesses is important for studying closure patterns)\n",
            "  4. No date range restriction (full temporal span needed for time-series)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get some counts\n",
        "\n",
        "- wanted to see counts per month to get a sense of the types of model and windows we would want for regression/time-series"
      ],
      "metadata": {
        "id": "8GhtDJ493y3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Safety - ensure datetime (EDA notebook already does this, but this won't hurt)\n",
        "rest_review_df['date'] = pd.to_datetime(rest_review_df['date'], errors='coerce')\n",
        "\n",
        "# 1) Overall monthly review counts\n",
        "monthly_counts = (\n",
        "    rest_review_df\n",
        "      .dropna(subset=['date'])\n",
        "      .groupby(rest_review_df['date'].dt.to_period('M'))\n",
        "      .size()\n",
        "      .rename('review_count')\n",
        "      .reset_index(name='review_count')\n",
        "      .rename(columns={'date': 'month'})\n",
        ")\n",
        "\n",
        "# Convert Period to timestamp for easy plotting/merging (month start)\n",
        "monthly_counts['month'] = monthly_counts['month'].dt.to_timestamp()\n",
        "\n",
        "print(monthly_counts.head(12))\n",
        "print('\\nRows:', len(monthly_counts))\n",
        "print('Date range:', monthly_counts['month'].min(), 'to', monthly_counts['month'].max())\n",
        "\n",
        "# 2) Monthly counts split by business status (Open vs Closed) - if you created 'status' in EDA\n",
        "if 'status' in rest_review_df.columns:\n",
        "    monthly_counts_by_status = (\n",
        "        rest_review_df\n",
        "          .dropna(subset=['date'])\n",
        "          .groupby([rest_review_df['date'].dt.to_period('M'), 'status'])\n",
        "          .size()\n",
        "          .rename('review_count')\n",
        "          .reset_index()\n",
        "          .rename(columns={'date': 'month'})\n",
        "    )\n",
        "    monthly_counts_by_status['month'] = monthly_counts_by_status['month'].dt.to_timestamp()\n",
        "    print('\\nBy status:')\n",
        "    print(monthly_counts_by_status.head(12))\n",
        "\n",
        "# 3) Monthly counts per business_id (useful for later time-series modeling)\n",
        "monthly_counts_by_business = (\n",
        "    rest_review_df\n",
        "      .dropna(subset=['date'])\n",
        "      .groupby(['business_id', rest_review_df['date'].dt.to_period('M')])\n",
        "      .size()\n",
        "      .rename('review_count')\n",
        "      .reset_index()\n",
        "      .rename(columns={'date': 'month'})\n",
        ")\n",
        "monthly_counts_by_business['month'] = monthly_counts_by_business['month'].dt.to_timestamp()\n",
        "\n",
        "print('\\nPer business:')\n",
        "print(monthly_counts_by_business.head(12))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELkDnhHo1TvB",
        "outputId": "b85ec676-9506-46d9-ac1b-d2901ea732b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        month  review_count\n",
            "0  2005-03-01             4\n",
            "1  2005-04-01             3\n",
            "2  2005-05-01             4\n",
            "3  2005-06-01             1\n",
            "4  2005-07-01            14\n",
            "5  2005-08-01             1\n",
            "6  2005-09-01             7\n",
            "7  2005-10-01             2\n",
            "8  2005-11-01             7\n",
            "9  2005-12-01             4\n",
            "10 2006-01-01            13\n",
            "11 2006-02-01             5\n",
            "\n",
            "Rows: 156\n",
            "Date range: 2005-03-01 00:00:00 to 2018-10-01 00:00:00\n",
            "\n",
            "By status:\n",
            "        month  status  review_count\n",
            "0  2005-03-01  Closed             1\n",
            "1  2005-03-01    Open             3\n",
            "2  2005-04-01  Closed             2\n",
            "3  2005-04-01    Open             1\n",
            "4  2005-05-01  Closed             2\n",
            "5  2005-05-01    Open             2\n",
            "6  2005-06-01    Open             1\n",
            "7  2005-07-01  Closed             7\n",
            "8  2005-07-01    Open             7\n",
            "9  2005-08-01    Open             1\n",
            "10 2005-09-01  Closed             3\n",
            "11 2005-09-01    Open             4\n",
            "\n",
            "Per business:\n",
            "               business_id      month  review_count\n",
            "0   --ZVrH2X2QXBFdCilbirsw 2013-07-01             1\n",
            "1   --ZVrH2X2QXBFdCilbirsw 2014-03-01             1\n",
            "2   --ZVrH2X2QXBFdCilbirsw 2014-12-01             1\n",
            "3   --ZVrH2X2QXBFdCilbirsw 2015-02-01             1\n",
            "4   --ZVrH2X2QXBFdCilbirsw 2015-05-01             1\n",
            "5   --ZVrH2X2QXBFdCilbirsw 2016-02-01             2\n",
            "6   --ZVrH2X2QXBFdCilbirsw 2016-03-01             1\n",
            "7   --ZVrH2X2QXBFdCilbirsw 2017-07-01             1\n",
            "8   --ZVrH2X2QXBFdCilbirsw 2018-02-01             1\n",
            "9   -1MhPXk1FglglUAmuPLIGg 2009-03-01             1\n",
            "10  -1MhPXk1FglglUAmuPLIGg 2010-05-01             1\n",
            "11  -1MhPXk1FglglUAmuPLIGg 2012-10-01             2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pivot for quick plot-ready table (month rows, status columns)\n",
        "if 'status' in rest_review_df.columns:\n",
        "    pivot = monthly_counts_by_status.pivot(index='month', columns='status', values='review_count').fillna(0).astype(int)\n",
        "    print(pivot.tail(12))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbzln-mc1e6U",
        "outputId": "1836c285-052f-4757-8788-043406922b1c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "status      Closed  Open\n",
            "month                   \n",
            "2017-11-01     129   529\n",
            "2017-12-01     181   904\n",
            "2018-01-01     141   652\n",
            "2018-02-01     202   888\n",
            "2018-03-01     220  1035\n",
            "2018-04-01     202  1012\n",
            "2018-05-01     177   924\n",
            "2018-06-01     144   844\n",
            "2018-07-01     182   967\n",
            "2018-08-01     137   799\n",
            "2018-09-01     105   749\n",
            "2018-10-01       8    38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ensure datetime\n",
        "rest_review_df['date'] = pd.to_datetime(rest_review_df['date'], errors='coerce')\n",
        "df = rest_review_df.dropna(subset=['date']).copy()\n",
        "\n",
        "# Ensure status exists (Open/Closed)\n",
        "if 'status' not in df.columns:\n",
        "    if 'is_open' in df.columns:\n",
        "        df['status'] = df['is_open'].map({1: 'Open', 0: 'Closed'})\n",
        "    else:\n",
        "        raise ValueError(\"Need either 'status' or 'is_open' in rest_review_df.\")\n",
        "\n",
        "# Build year-month grain counts (so the averaging is fair across years)\n",
        "df['year'] = df['date'].dt.year\n",
        "df['month_num'] = df['date'].dt.month\n",
        "df['month_name'] = df['date'].dt.strftime('%b')  # Jan, Feb, ...\n",
        "\n",
        "monthly_counts = (\n",
        "    df.groupby(['status', 'year', 'month_num', 'month_name'])\n",
        "      .size()\n",
        "      .reset_index(name='review_count')\n",
        ")\n",
        "\n",
        "# Average by calendar month across years\n",
        "avg_by_month = (\n",
        "    monthly_counts.groupby(['status', 'month_num', 'month_name'])['review_count']\n",
        "      .mean()\n",
        "      .reset_index(name='avg_reviews_per_month')\n",
        "      .sort_values(['month_num', 'status'])\n",
        ")\n",
        "\n",
        "# Nice pivot view (rows = month, cols = status)\n",
        "avg_by_month_pivot = (\n",
        "    avg_by_month.pivot(index=['month_num', 'month_name'], columns='status', values='avg_reviews_per_month')\n",
        "      .reset_index()\n",
        "      .sort_values('month_num')\n",
        ")\n",
        "\n",
        "print(avg_by_month_pivot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s25AFZaY2O1i",
        "outputId": "891a692e-ad1f-4a0d-efe4-2de227e2a972"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "status  month_num month_name      Closed        Open\n",
            "0               1        Jan  133.230769  343.076923\n",
            "1               2        Feb  159.300000  356.083333\n",
            "2               3        Mar  148.333333  392.833333\n",
            "3               4        Apr  121.538462  348.230769\n",
            "4               5        May  131.000000  358.285714\n",
            "5               6        Jun  118.307692  318.000000\n",
            "6               7        Jul  140.615385  404.461538\n",
            "7               8        Aug  142.692308  367.571429\n",
            "8               9        Sep  104.923077  320.833333\n",
            "9              10        Oct  135.692308  342.153846\n",
            "10             11        Nov  101.833333  273.000000\n",
            "11             12        Dec  119.000000  298.166667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the Business-Month Feature Table\n",
        "\n",
        "We start by creating a clean monthly view of the reviews dataset.  First, we convert each review timestamp into a month bucket (YYYY-MM) so we can measure activity and behavior at a consistent time grain.  \n",
        "\n",
        "We then compute:\n",
        "1. Total monthly review volume split by business status (Open vs Closed)\n",
        "2. A business-month feature table that aggregates review behavior for each restaurant each month (review count, average star rating, rating mix, engagement signals, and basic text length statistics).  \n",
        "\n",
        "This business-month table becomes the backbone for the modeling pipeline - we will later enrich it with neural-network sentiment scores and BERTopic topic proportions, then feed sequences of monthly features into a GRU/RNN to predict future sentiment direction and closure risk."
      ],
      "metadata": {
        "id": "66xmLcSU4ANu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = rest_review_df.copy()\n",
        "\n",
        "# Ensure datetime\n",
        "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "df = df.dropna(subset=['date'])\n",
        "\n",
        "# Month bucket (month start timestamp)\n",
        "df['month'] = df['date'].dt.to_period('M').dt.to_timestamp()\n",
        "\n",
        "# 1) Monthly totals by status (Open vs Closed)\n",
        "monthly_by_status = (\n",
        "    df.groupby(['status', 'month'])\n",
        "      .size()\n",
        "      .reset_index(name='review_count')\n",
        "      .sort_values(['month', 'status'])\n",
        ")\n",
        "\n",
        "print(monthly_by_status.head(24))\n",
        "\n",
        "# 2) Business-month backbone table (this is what the RNN will consume)\n",
        "biz_month = (\n",
        "    df.groupby(['business_id', 'status', 'month'])\n",
        "      .agg(\n",
        "          review_count=('review_id', 'count'),\n",
        "          avg_stars=('stars', 'mean'),\n",
        "          pct_1star=('stars', lambda s: (s <= 1.0).mean()),\n",
        "          pct_5star=('stars', lambda s: (s >= 5.0).mean()),\n",
        "          avg_useful=('useful', 'mean'),\n",
        "          avg_funny=('funny', 'mean'),\n",
        "          avg_cool=('cool', 'mean'),\n",
        "          avg_text_len=('text', lambda x: x.fillna('').str.len().mean()),\n",
        "          avg_word_count=('text', lambda x: x.fillna('').str.split().str.len().mean()),\n",
        "      )\n",
        "      .reset_index()\n",
        "      .sort_values(['business_id', 'month'])\n",
        ")\n",
        "\n",
        "print(biz_month.head(20))\n",
        "\n",
        "# 3) Optional: filter to businesses with enough activity for monthly sequences\n",
        "# Example rule: at least 12 total business-month rows in the dataset\n",
        "eligible = (\n",
        "    biz_month.groupby('business_id')['month']\n",
        "             .nunique()\n",
        "             .reset_index(name='n_months')\n",
        ")\n",
        "eligible_ids = eligible.loc[eligible['n_months'] >= 12, 'business_id']\n",
        "\n",
        "biz_month_eligible = biz_month[biz_month['business_id'].isin(eligible_ids)].copy()\n",
        "print(\"Eligible businesses:\", biz_month_eligible['business_id'].nunique())\n",
        "print(\"Eligible rows:\", len(biz_month_eligible))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0e-5KmW3_cF",
        "outputId": "1dad08f8-926b-4cc4-f1f6-69ecc6b05b04"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     status      month  review_count\n",
            "0    Closed 2005-03-01             1\n",
            "150    Open 2005-03-01             3\n",
            "1    Closed 2005-04-01             2\n",
            "151    Open 2005-04-01             1\n",
            "2    Closed 2005-05-01             2\n",
            "152    Open 2005-05-01             2\n",
            "153    Open 2005-06-01             1\n",
            "3    Closed 2005-07-01             7\n",
            "154    Open 2005-07-01             7\n",
            "155    Open 2005-08-01             1\n",
            "4    Closed 2005-09-01             3\n",
            "156    Open 2005-09-01             4\n",
            "5    Closed 2005-10-01             1\n",
            "157    Open 2005-10-01             1\n",
            "6    Closed 2005-11-01             2\n",
            "158    Open 2005-11-01             5\n",
            "7    Closed 2005-12-01             2\n",
            "159    Open 2005-12-01             2\n",
            "8    Closed 2006-01-01             7\n",
            "160    Open 2006-01-01             6\n",
            "9    Closed 2006-02-01             2\n",
            "161    Open 2006-02-01             3\n",
            "10   Closed 2006-03-01             4\n",
            "162    Open 2006-03-01             6\n",
            "               business_id  status      month  review_count  avg_stars  \\\n",
            "0   --ZVrH2X2QXBFdCilbirsw  Closed 2013-07-01             1        5.0   \n",
            "1   --ZVrH2X2QXBFdCilbirsw  Closed 2014-03-01             1        5.0   \n",
            "2   --ZVrH2X2QXBFdCilbirsw  Closed 2014-12-01             1        5.0   \n",
            "3   --ZVrH2X2QXBFdCilbirsw  Closed 2015-02-01             1        3.0   \n",
            "4   --ZVrH2X2QXBFdCilbirsw  Closed 2015-05-01             1        5.0   \n",
            "5   --ZVrH2X2QXBFdCilbirsw  Closed 2016-02-01             2        5.0   \n",
            "6   --ZVrH2X2QXBFdCilbirsw  Closed 2016-03-01             1        5.0   \n",
            "7   --ZVrH2X2QXBFdCilbirsw  Closed 2017-07-01             1        5.0   \n",
            "8   --ZVrH2X2QXBFdCilbirsw  Closed 2018-02-01             1        5.0   \n",
            "9   -1MhPXk1FglglUAmuPLIGg    Open 2009-03-01             1        3.0   \n",
            "10  -1MhPXk1FglglUAmuPLIGg    Open 2010-05-01             1        4.0   \n",
            "11  -1MhPXk1FglglUAmuPLIGg    Open 2012-10-01             2        4.5   \n",
            "12  -1MhPXk1FglglUAmuPLIGg    Open 2013-07-01             1        5.0   \n",
            "13  -1MhPXk1FglglUAmuPLIGg    Open 2013-10-01             1        1.0   \n",
            "14  -1MhPXk1FglglUAmuPLIGg    Open 2014-03-01             1        3.0   \n",
            "15  -1MhPXk1FglglUAmuPLIGg    Open 2014-11-01             1        2.0   \n",
            "16  -1MhPXk1FglglUAmuPLIGg    Open 2016-01-01             1        4.0   \n",
            "17  -1MhPXk1FglglUAmuPLIGg    Open 2016-03-01             1        5.0   \n",
            "18  -1MhPXk1FglglUAmuPLIGg    Open 2016-06-01             1        4.0   \n",
            "19  -1MhPXk1FglglUAmuPLIGg    Open 2017-05-01             1        5.0   \n",
            "\n",
            "    pct_1star  pct_5star  avg_useful  avg_funny  avg_cool  avg_text_len  \\\n",
            "0         0.0        1.0         1.0        1.0       1.0         310.0   \n",
            "1         0.0        1.0         0.0        1.0       2.0         154.0   \n",
            "2         0.0        1.0         0.0        0.0       0.0         475.0   \n",
            "3         0.0        0.0         0.0        0.0       0.0         279.0   \n",
            "4         0.0        1.0         1.0        0.0       0.0         420.0   \n",
            "5         0.0        1.0         0.0        0.0       0.0          68.5   \n",
            "6         0.0        1.0         1.0        0.0       1.0         474.0   \n",
            "7         0.0        1.0         1.0        0.0       0.0         323.0   \n",
            "8         0.0        1.0         0.0        0.0       0.0          89.0   \n",
            "9         0.0        0.0         2.0        0.0       2.0         183.0   \n",
            "10        0.0        0.0         1.0        0.0       1.0         504.0   \n",
            "11        0.0        0.5         2.0        1.0       1.0         465.0   \n",
            "12        0.0        1.0         2.0        1.0       1.0         355.0   \n",
            "13        1.0        0.0         2.0        1.0       0.0         261.0   \n",
            "14        0.0        0.0         0.0        0.0       0.0         832.0   \n",
            "15        0.0        0.0         0.0        0.0       0.0         609.0   \n",
            "16        0.0        0.0         0.0        0.0       0.0        1000.0   \n",
            "17        0.0        1.0         0.0        0.0       0.0         107.0   \n",
            "18        0.0        0.0         0.0        0.0       0.0         224.0   \n",
            "19        0.0        1.0         0.0        1.0       0.0         134.0   \n",
            "\n",
            "    avg_word_count  \n",
            "0             55.0  \n",
            "1             31.0  \n",
            "2             86.0  \n",
            "3             49.0  \n",
            "4             81.0  \n",
            "5             12.0  \n",
            "6             79.0  \n",
            "7             60.0  \n",
            "8             16.0  \n",
            "9             34.0  \n",
            "10           100.0  \n",
            "11            87.0  \n",
            "12            73.0  \n",
            "13            48.0  \n",
            "14           152.0  \n",
            "15           119.0  \n",
            "16           199.0  \n",
            "17            20.0  \n",
            "18            41.0  \n",
            "19            24.0  \n",
            "Eligible businesses: 1305\n",
            "Eligible rows: 35768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VADER comparison\n",
        "\n",
        "## 1) Create a baseline sentiment score (VADER) per review\n",
        "\n",
        "This gives you an immediate, cheap sentiment channel to compare against the NN later."
      ],
      "metadata": {
        "id": "q1IfKSPs5sy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "df = rest_review_df.copy()\n",
        "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "df = df.dropna(subset=['date'])\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# VADER scores\n",
        "vader = df['text'].fillna('').apply(analyzer.polarity_scores)\n",
        "df['vader_neg'] = vader.apply(lambda d: d['neg'])\n",
        "df['vader_neu'] = vader.apply(lambda d: d['neu'])\n",
        "df['vader_pos'] = vader.apply(lambda d: d['pos'])\n",
        "df['vader_compound'] = vader.apply(lambda d: d['compound'])\n",
        "\n",
        "df[['review_id','stars','vader_compound']].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CioLN3iD5sLL",
        "outputId": "fd9e4e0e-f030-43ac-9549-2bba4bed582b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                review_id  stars  vader_compound\n",
              "0  KU_O5udG6zpxOg-VcAEodg    3.0          0.8597\n",
              "2  saUsX_uimxRlCVr67Z4Jig    3.0          0.9201\n",
              "3  AqPFMleE6RsU23_auESxiA    5.0          0.9588\n",
              "4  Sx8TMOWLNuJBWer-0pcmoA    4.0          0.9815\n",
              "5  JrIxlS1TzJ-iCu79ul40cQ    1.0          0.7117"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f815d6ad-05de-4315-87f0-790444418101\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>vader_compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KU_O5udG6zpxOg-VcAEodg</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.8597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>saUsX_uimxRlCVr67Z4Jig</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.9201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AqPFMleE6RsU23_auESxiA</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.9588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.9815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>JrIxlS1TzJ-iCu79ul40cQ</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.7117</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f815d6ad-05de-4315-87f0-790444418101')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f815d6ad-05de-4315-87f0-790444418101 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f815d6ad-05de-4315-87f0-790444418101');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df[['review_id','stars','vader_compound']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"review_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"saUsX_uimxRlCVr67Z4Jig\",\n          \"JrIxlS1TzJ-iCu79ul40cQ\",\n          \"AqPFMleE6RsU23_auESxiA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stars\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.4832396974191326,\n        \"min\": 1.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          5.0,\n          1.0,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vader_compound\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.107998185169937,\n        \"min\": 0.7117,\n        \"max\": 0.9815,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.9201,\n          0.7117,\n          0.9588\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Aggregate VADER to business-month features\n",
        "\n",
        "This becomes part of the sequence input."
      ],
      "metadata": {
        "id": "fECZk09G5-ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['month'] = df['date'].dt.to_period('M').dt.to_timestamp()\n",
        "\n",
        "biz_month_vader = (\n",
        "    df.groupby(['business_id', 'status', 'month'])\n",
        "      .agg(\n",
        "          review_count=('review_id', 'count'),\n",
        "          avg_stars=('stars', 'mean'),\n",
        "          vader_mean=('vader_compound', 'mean'),\n",
        "          vader_std=('vader_compound', 'std'),\n",
        "          neg_share=('vader_compound', lambda s: (s < -0.05).mean()),\n",
        "          pos_share=('vader_compound', lambda s: (s >  0.05).mean()),\n",
        "      )\n",
        "      .reset_index()\n",
        "      .sort_values(['business_id', 'month'])\n",
        ")\n",
        "\n",
        "biz_month_vader.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "yD-bq2zv5_AU",
        "outputId": "b61a2493-bf6e-4bfd-874c-eb024f8ee6ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               business_id  status      month  review_count  avg_stars  \\\n",
              "0   --ZVrH2X2QXBFdCilbirsw  Closed 2013-07-01             1        5.0   \n",
              "1   --ZVrH2X2QXBFdCilbirsw  Closed 2014-03-01             1        5.0   \n",
              "2   --ZVrH2X2QXBFdCilbirsw  Closed 2014-12-01             1        5.0   \n",
              "3   --ZVrH2X2QXBFdCilbirsw  Closed 2015-02-01             1        3.0   \n",
              "4   --ZVrH2X2QXBFdCilbirsw  Closed 2015-05-01             1        5.0   \n",
              "5   --ZVrH2X2QXBFdCilbirsw  Closed 2016-02-01             2        5.0   \n",
              "6   --ZVrH2X2QXBFdCilbirsw  Closed 2016-03-01             1        5.0   \n",
              "7   --ZVrH2X2QXBFdCilbirsw  Closed 2017-07-01             1        5.0   \n",
              "8   --ZVrH2X2QXBFdCilbirsw  Closed 2018-02-01             1        5.0   \n",
              "9   -1MhPXk1FglglUAmuPLIGg    Open 2009-03-01             1        3.0   \n",
              "10  -1MhPXk1FglglUAmuPLIGg    Open 2010-05-01             1        4.0   \n",
              "11  -1MhPXk1FglglUAmuPLIGg    Open 2012-10-01             2        4.5   \n",
              "12  -1MhPXk1FglglUAmuPLIGg    Open 2013-07-01             1        5.0   \n",
              "13  -1MhPXk1FglglUAmuPLIGg    Open 2013-10-01             1        1.0   \n",
              "14  -1MhPXk1FglglUAmuPLIGg    Open 2014-03-01             1        3.0   \n",
              "15  -1MhPXk1FglglUAmuPLIGg    Open 2014-11-01             1        2.0   \n",
              "16  -1MhPXk1FglglUAmuPLIGg    Open 2016-01-01             1        4.0   \n",
              "17  -1MhPXk1FglglUAmuPLIGg    Open 2016-03-01             1        5.0   \n",
              "18  -1MhPXk1FglglUAmuPLIGg    Open 2016-06-01             1        4.0   \n",
              "19  -1MhPXk1FglglUAmuPLIGg    Open 2017-05-01             1        5.0   \n",
              "\n",
              "    vader_mean  vader_std  neg_share  pos_share  \n",
              "0       0.8856        NaN        0.0        1.0  \n",
              "1       0.7777        NaN        0.0        1.0  \n",
              "2       0.8646        NaN        0.0        1.0  \n",
              "3       0.8921        NaN        0.0        1.0  \n",
              "4       0.6468        NaN        0.0        1.0  \n",
              "5       0.8821   0.076085        0.0        1.0  \n",
              "6       0.9449        NaN        0.0        1.0  \n",
              "7       0.9794        NaN        0.0        1.0  \n",
              "8       0.0000        NaN        0.0        0.0  \n",
              "9       0.8020        NaN        0.0        1.0  \n",
              "10      0.8740        NaN        0.0        1.0  \n",
              "11      0.9195   0.085418        0.0        1.0  \n",
              "12      0.8398        NaN        0.0        1.0  \n",
              "13     -0.4767        NaN        1.0        0.0  \n",
              "14      0.9671        NaN        0.0        1.0  \n",
              "15      0.9846        NaN        0.0        1.0  \n",
              "16      0.9813        NaN        0.0        1.0  \n",
              "17      0.8975        NaN        0.0        1.0  \n",
              "18      0.7184        NaN        0.0        1.0  \n",
              "19      0.9665        NaN        0.0        1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7e810eb2-462d-434f-a815-8155de641827\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>status</th>\n",
              "      <th>month</th>\n",
              "      <th>review_count</th>\n",
              "      <th>avg_stars</th>\n",
              "      <th>vader_mean</th>\n",
              "      <th>vader_std</th>\n",
              "      <th>neg_share</th>\n",
              "      <th>pos_share</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>--ZVrH2X2QXBFdCilbirsw</td>\n",
              "      <td>Closed</td>\n",
              "      <td>2013-07-01</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.8856</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>--ZVrH2X2QXBFdCilbirsw</td>\n",
              "      <td>Closed</td>\n",
              "      <td>2014-03-01</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.7777</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>--ZVrH2X2QXBFdCilbirsw</td>\n",
              "      <td>Closed</td>\n",
              "      <td>2014-12-01</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.8646</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>--ZVrH2X2QXBFdCilbirsw</td>\n",
              "      <td>Closed</td>\n",
              "      <td>2015-02-01</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.8921</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>--ZVrH2X2QXBFdCilbirsw</td>\n",
              "      <td>Closed</td>\n",
              "      <td>2015-05-01</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.6468</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>--ZVrH2X2QXBFdCilbirsw</td>\n",
              "      <td>Closed</td>\n",
              "      <td>2016-02-01</td>\n",
              "      <td>2</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.8821</td>\n",
              "      <td>0.076085</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>--ZVrH2X2QXBFdCilbirsw</td>\n",
              "      <td>Closed</td>\n",
              "      <td>2016-03-01</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.9449</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>--ZVrH2X2QXBFdCilbirsw</td>\n",
              "      <td>Closed</td>\n",
              "      <td>2017-07-01</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.9794</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>--ZVrH2X2QXBFdCilbirsw</td>\n",
              "      <td>Closed</td>\n",
              "      <td>2018-02-01</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-1MhPXk1FglglUAmuPLIGg</td>\n",
              "      <td>Open</td>\n",
              "      <td>2009-03-01</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.8020</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-1MhPXk1FglglUAmuPLIGg</td>\n",
              "      <td>Open</td>\n",
              "      <td>2010-05-01</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.8740</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>-1MhPXk1FglglUAmuPLIGg</td>\n",
              "      <td>Open</td>\n",
              "      <td>2012-10-01</td>\n",
              "      <td>2</td>\n",
              "      <td>4.5</td>\n",
              "      <td>0.9195</td>\n",
              "      <td>0.085418</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>-1MhPXk1FglglUAmuPLIGg</td>\n",
              "      <td>Open</td>\n",
              "      <td>2013-07-01</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.8398</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>-1MhPXk1FglglUAmuPLIGg</td>\n",
              "      <td>Open</td>\n",
              "      <td>2013-10-01</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.4767</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-1MhPXk1FglglUAmuPLIGg</td>\n",
              "      <td>Open</td>\n",
              "      <td>2014-03-01</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.9671</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-1MhPXk1FglglUAmuPLIGg</td>\n",
              "      <td>Open</td>\n",
              "      <td>2014-11-01</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.9846</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>-1MhPXk1FglglUAmuPLIGg</td>\n",
              "      <td>Open</td>\n",
              "      <td>2016-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.9813</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>-1MhPXk1FglglUAmuPLIGg</td>\n",
              "      <td>Open</td>\n",
              "      <td>2016-03-01</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.8975</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-1MhPXk1FglglUAmuPLIGg</td>\n",
              "      <td>Open</td>\n",
              "      <td>2016-06-01</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.7184</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>-1MhPXk1FglglUAmuPLIGg</td>\n",
              "      <td>Open</td>\n",
              "      <td>2017-05-01</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.9665</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e810eb2-462d-434f-a815-8155de641827')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7e810eb2-462d-434f-a815-8155de641827 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7e810eb2-462d-434f-a815-8155de641827');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "biz_month_vader",
              "summary": "{\n  \"name\": \"biz_month_vader\",\n  \"rows\": 47905,\n  \"fields\": [\n    {\n      \"column\": \"business_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4132,\n        \"samples\": [\n          \"TB1FQ3iO9KJWOU56sA7Ozw\",\n          \"IazLGcO9aggJnMMa_5UO1Q\",\n          \"U30ggGzFpXvc2NZYwOW3qg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Open\",\n          \"Closed\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2005-03-01 00:00:00\",\n        \"max\": \"2018-10-01 00:00:00\",\n        \"num_unique_values\": 156,\n        \"samples\": [\n          \"2016-09-01 00:00:00\",\n          \"2011-05-01 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 24,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          1,\n          19\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_stars\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2717771558180513,\n        \"min\": 1.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 127,\n        \"samples\": [\n          4.6,\n          4.090909090909091\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vader_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.48262068547413706,\n        \"min\": -0.9933,\n        \"max\": 0.9997,\n        \"num_unique_values\": 15763,\n        \"samples\": [\n          -0.9558,\n          0.49654\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vader_std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3443965766846313,\n        \"min\": 0.0,\n        \"max\": 1.403394828620941,\n        \"num_unique_values\": 10831,\n        \"samples\": [\n          0.4123728450484262,\n          0.8621462935190126\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"neg_share\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3007239100530962,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 38,\n        \"samples\": [\n          0.2727272727272727,\n          0.5454545454545454\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pos_share\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3140890615385819,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 42,\n        \"samples\": [\n          0.5714285714285714,\n          0.8571428571428571\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "In this section we fine-tune a transformer-based sentiment model on our own Yelp review data to produce a high-quality, domain-specific sentiment signal.  We create a supervised training set using clearly polarized reviews (1-star = negative, 5-star = positive), split it into train/validation sets, and fine-tune DistilBERT to classify review sentiment.  After training, we use the best checkpoint to score every review with a continuous probability of positive sentiment (0-1).  Finally, we aggregate these transformer sentiment scores to the business-month level (mean, variability, and positive/negative share), creating time-series features that will later feed our GRU model for forecasting sentiment trajectories and predicting closure risk."
      ],
      "metadata": {
        "id": "rrRVWIZVEGIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U transformers datasets accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmGIyloeLrlr",
        "outputId": "e74cd6c4-05ca-4ce5-bc2d-08e57b04bd47"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/10.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/10.4 MB\u001b[0m \u001b[31m158.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/10.4 MB\u001b[0m \u001b[31m158.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/10.4 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Cell 0 - Setup + Imports\n",
        "# ============================\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# Cell 1 - Reproducibility\n",
        "# ============================\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "# ============================\n",
        "# Cell 2 - Prep Data\n",
        "# Assumes rest_review_df is already loaded like your EDA notebook.\n",
        "# ============================\n",
        "df = rest_review_df.copy()\n",
        "\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"date\"]).copy()\n",
        "\n",
        "df[\"text\"] = df[\"text\"].fillna(\"\").astype(str)\n",
        "\n",
        "# Fine-tune labels: 1-star = 0 (negative), 5-star = 1 (positive)\n",
        "train_df = df[df[\"stars\"].isin([1.0, 5.0])].copy()\n",
        "train_df[\"label\"] = (train_df[\"stars\"] == 5.0).astype(int)\n",
        "\n",
        "\n",
        "train_df = df[df[\"stars\"].isin([1.0, 2.0, 4.0, 5.0])].copy()\n",
        "train_df[\"label\"] = train_df[\"stars\"].isin([4.0, 5.0]).astype(int)\n",
        "\n",
        "print(\"Fine-tune rows:\", len(train_df))\n",
        "print(train_df[\"label\"].value_counts())\n",
        "\n",
        "# ============================\n",
        "# Cell 3 - Stratified Train/Val Split (no sklearn)\n",
        "# ============================\n",
        "y = train_df[\"label\"].values\n",
        "\n",
        "idx_pos = np.where(y == 1)[0]\n",
        "idx_neg = np.where(y == 0)[0]\n",
        "\n",
        "np.random.shuffle(idx_pos)\n",
        "np.random.shuffle(idx_neg)\n",
        "\n",
        "split_pos = int(0.8 * len(idx_pos))\n",
        "split_neg = int(0.8 * len(idx_neg))\n",
        "\n",
        "tr_idx = np.concatenate([idx_pos[:split_pos], idx_neg[:split_neg]])\n",
        "va_idx = np.concatenate([idx_pos[split_pos:], idx_neg[split_neg:]])\n",
        "\n",
        "np.random.shuffle(tr_idx)\n",
        "np.random.shuffle(va_idx)\n",
        "\n",
        "train_split = train_df.iloc[tr_idx].reset_index(drop=True)\n",
        "val_split = train_df.iloc[va_idx].reset_index(drop=True)\n",
        "\n",
        "print(\"Train split:\", len(train_split), \"Val split:\", len(val_split))\n",
        "print(\"Train label dist:\\n\", train_split[\"label\"].value_counts(normalize=True))\n",
        "print(\"Val label dist:\\n\", val_split[\"label\"].value_counts(normalize=True))\n",
        "\n",
        "# ============================\n",
        "# Cell 4 - Tokenize + Build HF Datasets\n",
        "# ============================\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "MAX_LEN = 256\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN\n",
        "    )\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_split[[\"text\", \"label\"]])\n",
        "val_ds = Dataset.from_pandas(val_split[[\"text\", \"label\"]])\n",
        "\n",
        "train_ds = train_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
        "val_ds = val_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# ============================\n",
        "# Cell 5 - Model + Metrics\n",
        "# ============================\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "\n",
        "    tp = int(((preds == 1) & (labels == 1)).sum())\n",
        "    tn = int(((preds == 0) & (labels == 0)).sum())\n",
        "    fp = int(((preds == 1) & (labels == 0)).sum())\n",
        "    fn = int(((preds == 0) & (labels == 1)).sum())\n",
        "\n",
        "    acc = (tp + tn) / max(1, tp + tn + fp + fn)\n",
        "    precision = tp / max(1, tp + fp)\n",
        "    recall = tp / max(1, tp + fn)\n",
        "    f1 = 2 * precision * recall / max(1e-12, (precision + recall))\n",
        "\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "# ============================\n",
        "# Cell 6 - TrainingArguments (Transformers v5-safe) + Trainer + Train\n",
        "# ============================\n",
        "OUT_DIR = \"../artifacts/transformer_sentiment_distilbert\"\n",
        "\n",
        "EPOCHS = 3\n",
        "PER_DEVICE_TRAIN_BS = 16\n",
        "PER_DEVICE_EVAL_BS = 32\n",
        "GRAD_ACCUM = 2\n",
        "\n",
        "# warmup_ratio is deprecated in v5.2 - use warmup_steps now.\n",
        "steps_per_epoch = math.ceil(len(train_ds) / (PER_DEVICE_TRAIN_BS * GRAD_ACCUM))\n",
        "total_steps = steps_per_epoch * EPOCHS\n",
        "warmup_steps = int(0.06 * total_steps)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUT_DIR,\n",
        "    seed=SEED,\n",
        "\n",
        "    # Transformers v5 uses eval_strategy (NOT evaluation_strategy)\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "\n",
        "    num_train_epochs=EPOCHS,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=PER_DEVICE_TRAIN_BS,\n",
        "    per_device_eval_batch_size=PER_DEVICE_EVAL_BS,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "\n",
        "    warmup_steps=warmup_steps,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    logging_steps=100,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Transformers v5 Trainer does NOT accept tokenizer=.  Use processing_class=.\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=data_collator,\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "eval_metrics = trainer.evaluate()\n",
        "print(\"Eval metrics:\", eval_metrics)\n",
        "\n",
        "trainer.save_model(OUT_DIR)\n",
        "tokenizer.save_pretrained(OUT_DIR)\n",
        "print(\"Saved to:\", OUT_DIR)\n",
        "\n",
        "# ============================\n",
        "# Cell 7 - Temperature scaling (calibration) on validation set\n",
        "# Output: best_T\n",
        "# ============================\n",
        "pred_out = trainer.predict(val_ds)\n",
        "val_logits = torch.tensor(pred_out.predictions, dtype=torch.float32)\n",
        "val_labels = torch.tensor(pred_out.label_ids, dtype=torch.long)\n",
        "\n",
        "def nll_for_T(T: float) -> float:\n",
        "    scaled = val_logits / T\n",
        "    probs = torch.softmax(scaled, dim=1)\n",
        "    p = probs[torch.arange(len(val_labels)), val_labels]\n",
        "    return (-torch.log(p.clamp_min(1e-12))).mean().item()\n",
        "\n",
        "Ts = np.linspace(0.5, 5.0, 46)  # 0.5, 0.6, ..., 5.0\n",
        "losses = [nll_for_T(float(T)) for T in Ts]\n",
        "best_T = float(Ts[int(np.argmin(losses))])\n",
        "\n",
        "print(\"Best temperature:\", best_T)\n",
        "print(\"NLL @ best_T:\", min(losses))\n",
        "\n",
        "# ============================\n",
        "# Cell 8 - Score ALL Reviews (fast batch logits, temperature-scaled)\n",
        "# Output: df['tx_sent'] = calibrated P(positive), 0..1\n",
        "# ============================\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "ft_model = AutoModelForSequenceClassification.from_pretrained(OUT_DIR)\n",
        "ft_tokenizer = AutoTokenizer.from_pretrained(OUT_DIR)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "ft_model.to(device)\n",
        "ft_model.eval()\n",
        "\n",
        "collator = DataCollatorWithPadding(tokenizer=ft_tokenizer, return_tensors=\"pt\")\n",
        "\n",
        "texts = df[\"text\"].fillna(\"\").astype(str).tolist()\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "p_pos = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(texts), BATCH_SIZE):\n",
        "        batch_texts = texts[i:i + BATCH_SIZE]\n",
        "\n",
        "        enc = ft_tokenizer(\n",
        "            batch_texts,\n",
        "            truncation=True,\n",
        "            max_length=MAX_LEN\n",
        "        )\n",
        "\n",
        "        features = [{k: enc[k][j] for k in enc.keys()} for j in range(len(batch_texts))]\n",
        "        batch = collator(features)\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        logits = ft_model(**batch).logits\n",
        "        probs = torch.softmax(logits / best_T, dim=1)[:, 1].detach().cpu().numpy()\n",
        "        p_pos.extend(probs.tolist())\n",
        "\n",
        "df[\"tx_sent\"] = np.array(p_pos, dtype=float)\n",
        "\n",
        "print(df[[\"review_id\", \"stars\", \"tx_sent\"]].head(10))\n",
        "print(\"tx_sent range:\", df[\"tx_sent\"].min(), \"to\", df[\"tx_sent\"].max())\n",
        "\n",
        "# ============================\n",
        "# Cell 9 - Aggregate to Business-Month Features (feeds GRU time-series later)\n",
        "# Output: biz_month_tx\n",
        "# ============================\n",
        "df[\"month\"] = df[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
        "\n",
        "\"\"\"\n",
        "biz_month_tx = (\n",
        "    df.groupby([\"business_id\", \"status\", \"month\"])\n",
        "      .agg(\n",
        "          review_count=(\"review_id\", \"count\"),\n",
        "          avg_stars=(\"stars\", \"mean\"),\n",
        "          tx_sent_mean=(\"tx_sent\", \"mean\"),\n",
        "          tx_sent_std=(\"tx_sent\", \"std\"),\n",
        "          tx_neg_share=(\"tx_sent\", lambda s: (s < 0.30).mean()),\n",
        "          tx_pos_share=(\"tx_sent\", lambda s: (s > 0.70).mean()),\n",
        "      )\n",
        "      .reset_index()\n",
        "      .sort_values([\"business_id\", \"month\"])\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "MIN_N = 5\n",
        "\n",
        "biz_month_tx[\"tx_pos_share\"] = np.where(\n",
        "    biz_month_tx[\"review_count\"] >= MIN_N,\n",
        "    biz_month_tx[\"tx_pos_share\"],\n",
        "    np.nan\n",
        ")\n",
        "\n",
        "biz_month_tx[\"tx_neg_share\"] = np.where(\n",
        "    biz_month_tx[\"review_count\"] >= MIN_N,\n",
        "    biz_month_tx[\"tx_neg_share\"],\n",
        "    np.nan\n",
        ")\n",
        "\n",
        "# std is NaN when review_count == 1 - make it numeric for modeling\n",
        "biz_month_tx[\"tx_sent_std\"] = biz_month_tx[\"tx_sent_std\"].fillna(0.0)\n",
        "\n",
        "print(biz_month_tx.head(20))\n",
        "print(\"Rows:\", len(biz_month_tx), \"Businesses:\", biz_month_tx[\"business_id\"].nunique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "5b59EFriU66f",
        "outputId": "a4bde4ef-dda8-4a8f-e86f-c8e7b42f6c06"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "pyarrow.lib.IpcReadOptions size changed, may indicate binary incompatibility. Expected 112 from C header, got 104 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-876572602.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m from transformers import (\n\u001b[1;32m     13\u001b[0m     \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"4.5.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptimizedTypedSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msanitize_patterns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_reader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthread_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/parquet/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# flake8: noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     raise ImportError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/_parquet.pyx\u001b[0m in \u001b[0;36minit pyarrow._parquet\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: pyarrow.lib.IpcReadOptions size changed, may indicate binary incompatibility. Expected 112 from C header, got 104 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "biz_month_tx[\"tx_neg_share\"] = biz_month_tx[\"tx_neg_share\"].fillna(0.5)\n",
        "biz_month_tx[\"tx_pos_share\"] = biz_month_tx[\"tx_pos_share\"].fillna(0.5)"
      ],
      "metadata": {
        "id": "btUmJ0j3xkZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, pyarrow, datasets\n",
        "print(\"python:\", sys.version)\n",
        "print(\"pyarrow:\", pyarrow.__version__, pyarrow.__file__)\n",
        "print(\"datasets:\", datasets.__version__)"
      ],
      "metadata": {
        "id": "GLUTtkIzikNg",
        "outputId": "6ad490f8-e5d7-455c-dcda-11f286fda79f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "pyarrow.lib.IpcReadOptions size changed, may indicate binary incompatibility. Expected 112 from C header, got 104 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4197071131.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"python:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datasets:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"4.5.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptimizedTypedSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msanitize_patterns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_reader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthread_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/parquet/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# flake8: noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     raise ImportError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/_parquet.pyx\u001b[0m in \u001b[0;36minit pyarrow._parquet\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: pyarrow.lib.IpcReadOptions size changed, may indicate binary incompatibility. Expected 112 from C header, got 104 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU"
      ],
      "metadata": {
        "id": "tp67iykNySq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Cell 10 - Build GRU-ready sequences (business-month time series)\n",
        "# Goal: create X (sequences) + y (closure label) for modeling.\n",
        "# Output: X (N, SEQ_LEN, F), y (N,), meta_df (one row per sequence)\n",
        "# ============================\n",
        "\n",
        "# ---- knobs ----\n",
        "SEQ_LEN = 12          # months per sequence (1 year)\n",
        "MIN_MONTHS = SEQ_LEN  # require at least SEQ_LEN months of history\n",
        "TARGET_HORIZON = 1    # predict next month (we’ll label sequences by business status)\n",
        "FEATURE_COLS = [\n",
        "    \"review_count\",\n",
        "    \"avg_stars\",\n",
        "    \"tx_sent_mean\",\n",
        "    \"tx_sent_std\",\n",
        "    \"tx_neg_share\",\n",
        "    \"tx_pos_share\",\n",
        "]\n",
        "\n",
        "# ---- safety checks ----\n",
        "missing = [c for c in FEATURE_COLS if c not in biz_month_tx.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required feature columns in biz_month_tx: {missing}\")\n",
        "\n",
        "# Ensure sorted\n",
        "biz_month_tx = biz_month_tx.sort_values([\"business_id\", \"month\"]).reset_index(drop=True)\n",
        "\n",
        "# Make sure numeric dtypes (GRU hates objects)\n",
        "for c in FEATURE_COLS:\n",
        "    biz_month_tx[c] = pd.to_numeric(biz_month_tx[c], errors=\"coerce\")\n",
        "\n",
        "# Fill any remaining NaNs defensively (should be minimal given your prior fills)\n",
        "biz_month_tx[FEATURE_COLS] = biz_month_tx[FEATURE_COLS].fillna(0.0)\n",
        "\n",
        "# ---- per-business normalization (recommended for sequences) ----\n",
        "# This avoids one mega-popular restaurant dominating scale.\n",
        "# We z-score each feature within each business.\n",
        "def zscore_group(g: pd.DataFrame) -> pd.DataFrame:\n",
        "    x = g[FEATURE_COLS].astype(float)\n",
        "    mu = x.mean(axis=0)\n",
        "    sd = x.std(axis=0).replace(0.0, 1.0)\n",
        "    g[[f\"{c}_z\" for c in FEATURE_COLS]] = (x - mu) / sd\n",
        "    return g\n",
        "\n",
        "biz_month_tx = biz_month_tx.groupby(\"business_id\", group_keys=False).apply(zscore_group)\n",
        "\n",
        "FEATURE_Z = [f\"{c}_z\" for c in FEATURE_COLS]\n",
        "\n",
        "# ---- build sequences ----\n",
        "X_list = []\n",
        "y_list = []\n",
        "meta = []\n",
        "\n",
        "# Label rule for now:\n",
        "# y = 1 if business is Closed, else 0 (Open).\n",
        "# (We’ll refine later to \"predict closure in next k months\".)\n",
        "status_map = {\"Closed\": 1, \"Open\": 0}\n",
        "\n",
        "for bid, g in biz_month_tx.groupby(\"business_id\"):\n",
        "    g = g.sort_values(\"month\").reset_index(drop=True)\n",
        "\n",
        "    # Need known status\n",
        "    if g[\"status\"].iloc[0] not in status_map:\n",
        "        continue\n",
        "\n",
        "    y_business = status_map[g[\"status\"].iloc[0]]\n",
        "\n",
        "    if len(g) < MIN_MONTHS:\n",
        "        continue\n",
        "\n",
        "    # Sliding windows: every possible SEQ_LEN-month chunk\n",
        "    for start in range(0, len(g) - SEQ_LEN - TARGET_HORIZON + 1):\n",
        "        end = start + SEQ_LEN\n",
        "        window = g.iloc[start:end]\n",
        "\n",
        "        X_seq = window[FEATURE_Z].to_numpy(dtype=np.float32)\n",
        "\n",
        "        X_list.append(X_seq)\n",
        "        y_list.append(y_business)\n",
        "        meta.append({\n",
        "            \"business_id\": bid,\n",
        "            \"status\": g[\"status\"].iloc[0],\n",
        "            \"start_month\": window[\"month\"].iloc[0],\n",
        "            \"end_month\": window[\"month\"].iloc[-1],\n",
        "            \"seq_len\": SEQ_LEN,\n",
        "        })\n",
        "\n",
        "X = np.stack(X_list, axis=0) if len(X_list) else np.empty((0, SEQ_LEN, len(FEATURE_Z)), dtype=np.float32)\n",
        "y = np.array(y_list, dtype=np.int64)\n",
        "meta_df = pd.DataFrame(meta)\n",
        "\n",
        "print(\"X shape:\", X.shape)  # (num_sequences, SEQ_LEN, num_features)\n",
        "print(\"y shape:\", y.shape)\n",
        "print(\"Class balance (y):\", pd.Series(y).value_counts())\n",
        "print(meta_df.head(10))\n",
        "\n",
        "# ============================\n",
        "# Cell 11 - Train/Val split at BUSINESS level (no leakage)\n",
        "# Output: X_train, y_train, X_val, y_val\n",
        "# ============================\n",
        "SEED = 42\n",
        "rng = np.random.default_rng(SEED)\n",
        "\n",
        "unique_biz = meta_df[\"business_id\"].unique()\n",
        "rng.shuffle(unique_biz)\n",
        "\n",
        "split = int(0.8 * len(unique_biz))\n",
        "train_biz = set(unique_biz[:split])\n",
        "val_biz   = set(unique_biz[split:])\n",
        "\n",
        "train_mask = meta_df[\"business_id\"].isin(train_biz).values\n",
        "val_mask   = meta_df[\"business_id\"].isin(val_biz).values\n",
        "\n",
        "X_train, y_train = X[train_mask], y[train_mask]\n",
        "X_val, y_val     = X[val_mask], y[val_mask]\n",
        "\n",
        "print(\"Train sequences:\", X_train.shape[0], \"Val sequences:\", X_val.shape[0])\n",
        "print(\"Train y balance:\\n\", pd.Series(y_train).value_counts(normalize=True))\n",
        "print(\"Val y balance:\\n\", pd.Series(y_val).value_counts(normalize=True))\n",
        "\n",
        "# ============================\n",
        "# Cell 12 - GRU model (Keras) for closure classification\n",
        "# Output: trained GRU model + history\n",
        "# ============================\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "N_FEATS = X_train.shape[2]\n",
        "\n",
        "model_gru = keras.Sequential([\n",
        "    layers.Input(shape=(SEQ_LEN, N_FEATS)),\n",
        "    layers.GRU(64, return_sequences=False),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(1, activation=\"sigmoid\"),\n",
        "])\n",
        "\n",
        "model_gru.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\n",
        "        keras.metrics.BinaryAccuracy(name=\"acc\"),\n",
        "        keras.metrics.Precision(name=\"precision\"),\n",
        "        keras.metrics.Recall(name=\"recall\"),\n",
        "        keras.metrics.AUC(name=\"auc\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "# handle imbalance via class weights (Closed is usually minority)\n",
        "pos = float((y_train == 1).sum())\n",
        "neg = float((y_train == 0).sum())\n",
        "w0 = (pos + neg) / max(1.0, 2.0 * neg)\n",
        "w1 = (pos + neg) / max(1.0, 2.0 * pos)\n",
        "class_weight = {0: w0, 1: w1}\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=3, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", factor=0.5, patience=2, min_lr=1e-5),\n",
        "]\n",
        "\n",
        "history = model_gru.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=128,\n",
        "    class_weight=class_weight,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "val_metrics = model_gru.evaluate(X_val, y_val, verbose=0)\n",
        "print(dict(zip(model_gru.metrics_names, val_metrics)))\n",
        "\n",
        "# ============================\n",
        "# Cell 13 - Quick sanity check: predicted risk examples\n",
        "# Output: top-risk businesses (by avg predicted probability over their sequences)\n",
        "# ============================\n",
        "val_probs = model_gru.predict(X_val, batch_size=256).reshape(-1)\n",
        "\n",
        "val_meta = meta_df.loc[val_mask].copy()\n",
        "val_meta[\"p_closed\"] = val_probs\n",
        "\n",
        "biz_risk = (\n",
        "    val_meta.groupby([\"business_id\", \"status\"])[\"p_closed\"]\n",
        "      .mean()\n",
        "      .reset_index()\n",
        "      .sort_values(\"p_closed\", ascending=False)\n",
        ")\n",
        "\n",
        "print(biz_risk.head(20))\n",
        "print(biz_risk.tail(20))\n",
        "\n"
      ],
      "metadata": {
        "id": "cbtjQ-iXyTt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Cell 14 - Create closure_month proxy per business\n",
        "# For Closed businesses, closure_month = last review month we observed.\n",
        "# ============================\n",
        "biz_last_month = (\n",
        "    biz_month_tx.groupby([\"business_id\", \"status\"])[\"month\"]\n",
        "      .max()\n",
        "      .reset_index()\n",
        "      .rename(columns={\"month\": \"last_review_month\"})\n",
        ")\n",
        "\n",
        "biz_last_month[\"closure_month\"] = pd.NaT\n",
        "biz_last_month.loc[biz_last_month[\"status\"] == \"Closed\", \"closure_month\"] = biz_last_month.loc[\n",
        "    biz_last_month[\"status\"] == \"Closed\", \"last_review_month\"\n",
        "].values\n",
        "\n",
        "print(biz_last_month.head(10))\n",
        "print(\"Closed businesses:\", (biz_last_month[\"status\"] == \"Closed\").sum())\n",
        "print(\"Open businesses:\", (biz_last_month[\"status\"] == \"Open\").sum())\n",
        "\n",
        "# ============================\n",
        "# Cell 15 - Rebuild sequences with a proper horizon label\n",
        "# y = 1 if business closes within H months AFTER the window ends\n",
        "# ============================\n",
        "SEQ_LEN = 12\n",
        "H = 6  # predict closure within next 6 months\n",
        "\n",
        "FEATURE_COLS = [\n",
        "    \"review_count\",\n",
        "    \"avg_stars\",\n",
        "    \"tx_sent_mean\",\n",
        "    \"tx_sent_std\",\n",
        "    \"tx_neg_share\",\n",
        "    \"tx_pos_share\",\n",
        "]\n",
        "\n",
        "# Ensure sorted\n",
        "biz_month_tx = biz_month_tx.sort_values([\"business_id\", \"month\"]).reset_index(drop=True)\n",
        "\n",
        "# Merge closure_month onto each row\n",
        "biz_month_tx2 = biz_month_tx.merge(\n",
        "    biz_last_month[[\"business_id\", \"status\", \"closure_month\"]],\n",
        "    on=[\"business_id\", \"status\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Build z-scored features per business again (same as before)\n",
        "def zscore_group(g: pd.DataFrame) -> pd.DataFrame:\n",
        "    x = g[FEATURE_COLS].astype(float)\n",
        "    mu = x.mean(axis=0)\n",
        "    sd = x.std(axis=0).replace(0.0, 1.0)\n",
        "    g[[f\"{c}_z\" for c in FEATURE_COLS]] = (x - mu) / sd\n",
        "    return g\n",
        "\n",
        "biz_month_tx2 = biz_month_tx2.groupby(\"business_id\", group_keys=False).apply(zscore_group)\n",
        "FEATURE_Z = [f\"{c}_z\" for c in FEATURE_COLS]\n",
        "\n",
        "# Activity filter: require at least SOME months with reviews in the window\n",
        "MIN_ACTIVE_MONTHS = 6  # out of 12\n",
        "MIN_REVIEWS_IN_WINDOW = 10\n",
        "\n",
        "X_list, y_list, meta = [], [], []\n",
        "\n",
        "for bid, g in biz_month_tx2.groupby(\"business_id\"):\n",
        "    g = g.sort_values(\"month\").reset_index(drop=True)\n",
        "    status = g[\"status\"].iloc[0]\n",
        "    closure_month = g[\"closure_month\"].iloc[0]\n",
        "\n",
        "    if len(g) < SEQ_LEN:\n",
        "        continue\n",
        "\n",
        "    for start in range(0, len(g) - SEQ_LEN + 1):\n",
        "        end = start + SEQ_LEN\n",
        "        window = g.iloc[start:end].copy()\n",
        "        window_end = window[\"month\"].iloc[-1]\n",
        "\n",
        "        # window activity filters\n",
        "        active_months = int((window[\"review_count\"] > 0).sum())\n",
        "        total_reviews = float(window[\"review_count\"].sum())\n",
        "        if active_months < MIN_ACTIVE_MONTHS:\n",
        "            continue\n",
        "        if total_reviews < MIN_REVIEWS_IN_WINDOW:\n",
        "            continue\n",
        "\n",
        "        # horizon label\n",
        "        if status == \"Open\" or pd.isna(closure_month):\n",
        "            y_seq = 0\n",
        "        else:\n",
        "            # label 1 if closure happens within next H months after window_end\n",
        "            # and window_end is before closure (avoid post-closure nonsense)\n",
        "            y_seq = int((window_end < closure_month) and (window_end >= (closure_month - pd.DateOffset(months=H))))\n",
        "\n",
        "        X_seq = window[FEATURE_Z].to_numpy(dtype=np.float32)\n",
        "\n",
        "        X_list.append(X_seq)\n",
        "        y_list.append(y_seq)\n",
        "        meta.append({\n",
        "            \"business_id\": bid,\n",
        "            \"status\": status,\n",
        "            \"start_month\": window[\"month\"].iloc[0],\n",
        "            \"end_month\": window_end,\n",
        "            \"closure_month\": closure_month,\n",
        "            \"y\": y_seq\n",
        "        })\n",
        "\n",
        "X2 = np.stack(X_list, axis=0) if len(X_list) else np.empty((0, SEQ_LEN, len(FEATURE_Z)), dtype=np.float32)\n",
        "y2 = np.array(y_list, dtype=np.int64)\n",
        "meta2 = pd.DataFrame(meta)\n",
        "\n",
        "print(\"X2 shape:\", X2.shape)\n",
        "print(\"y2 balance:\", pd.Series(y2).value_counts())\n",
        "print(meta2.head(10))\n",
        "\n",
        "# ============================\n",
        "# Cell 16 - Business-level split again (no leakage)\n",
        "# ============================\n",
        "rng = np.random.default_rng(42)\n",
        "unique_biz = meta2[\"business_id\"].unique()\n",
        "rng.shuffle(unique_biz)\n",
        "\n",
        "split = int(0.8 * len(unique_biz))\n",
        "train_biz = set(unique_biz[:split])\n",
        "val_biz   = set(unique_biz[split:])\n",
        "\n",
        "train_mask = meta2[\"business_id\"].isin(train_biz).values\n",
        "val_mask   = meta2[\"business_id\"].isin(val_biz).values\n",
        "\n",
        "X_train2, y_train2 = X2[train_mask], y2[train_mask]\n",
        "X_val2, y_val2     = X2[val_mask], y2[val_mask]\n",
        "\n",
        "print(\"Train sequences:\", X_train2.shape[0], \"Val sequences:\", X_val2.shape[0])\n",
        "print(\"Train y balance:\\n\", pd.Series(y_train2).value_counts(normalize=True))\n",
        "print(\"Val y balance:\\n\", pd.Series(y_val2).value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "LyD18nBBzDIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Cell 17 - GRU (imbalance-safe) + Train\n",
        "# Uses weighted BCE with pos_weight = neg/pos\n",
        "# ============================\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "SEQ_LEN = X_train2.shape[1]\n",
        "N_FEATS = X_train2.shape[2]\n",
        "\n",
        "pos = float((y_train2 == 1).sum())\n",
        "neg = float((y_train2 == 0).sum())\n",
        "pos_weight = neg / max(1.0, pos)\n",
        "\n",
        "print(\"Train pos:\", int(pos), \"Train neg:\", int(neg), \"pos_weight:\", pos_weight)\n",
        "\n",
        "def weighted_bce(y_true, y_pred):\n",
        "    # y_true: (batch,)\n",
        "    # y_pred: (batch,)\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    eps = tf.keras.backend.epsilon()\n",
        "    y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n",
        "\n",
        "    # BCE = -(y*log(p) + (1-y)*log(1-p))\n",
        "    # Weight positives by pos_weight\n",
        "    loss = -(pos_weight * y_true * tf.math.log(y_pred) + (1.0 - y_true) * tf.math.log(1.0 - y_pred))\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "model_gru = keras.Sequential([\n",
        "    layers.Input(shape=(SEQ_LEN, N_FEATS)),\n",
        "    layers.GRU(96, return_sequences=False),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Dense(48, activation=\"relu\"),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Dense(1, activation=\"sigmoid\"),\n",
        "])\n",
        "\n",
        "model_gru.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=weighted_bce,\n",
        "    metrics=[\n",
        "        keras.metrics.AUC(name=\"auc\"),\n",
        "        keras.metrics.Precision(name=\"precision\"),\n",
        "        keras.metrics.Recall(name=\"recall\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=4, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", factor=0.5, patience=2, min_lr=1e-5),\n",
        "]\n",
        "\n",
        "history = model_gru.fit(\n",
        "    X_train2, y_train2,\n",
        "    validation_data=(X_val2, y_val2),\n",
        "    epochs=30,\n",
        "    batch_size=256,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "val_metrics = model_gru.evaluate(X_val2, y_val2, verbose=0)\n",
        "print(dict(zip(model_gru.metrics_names, val_metrics)))\n",
        "\n",
        "# ============================\n",
        "# Cell 18 - Threshold sweep (pick operating point)\n",
        "# ============================\n",
        "import numpy as np\n",
        "\n",
        "probs = model_gru.predict(X_val2, batch_size=512).reshape(-1)\n",
        "ytrue = y_val2.astype(int)\n",
        "\n",
        "thresholds = np.linspace(0.05, 0.95, 19)\n",
        "\n",
        "rows = []\n",
        "for t in thresholds:\n",
        "    pred = (probs >= t).astype(int)\n",
        "    tp = int(((pred == 1) & (ytrue == 1)).sum())\n",
        "    fp = int(((pred == 1) & (ytrue == 0)).sum())\n",
        "    fn = int(((pred == 0) & (ytrue == 1)).sum())\n",
        "\n",
        "    precision = tp / max(1, (tp + fp))\n",
        "    recall    = tp / max(1, (tp + fn))\n",
        "    f1        = (2 * precision * recall) / max(1e-12, (precision + recall))\n",
        "    rows.append((t, tp, fp, fn, precision, recall, f1))\n",
        "\n",
        "print(\"thr  tp  fp  fn  precision  recall  f1\")\n",
        "for r in rows:\n",
        "    print(f\"{r[0]:.2f}  {r[1]:4d} {r[2]:4d} {r[3]:4d}   {r[4]:.3f}     {r[5]:.3f}  {r[6]:.3f}\")"
      ],
      "metadata": {
        "id": "2kjGezRm1K10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Cell 19 - Workload-based evaluation (Top-K% triage) - FIXED for your variables\n",
        "# Run AFTER Cell 16 (val_mask exists) and AFTER Cell 17/18 (model_gru trained).\n",
        "# Requires: model_gru, X_val2, y_val2, meta2, val_mask\n",
        "# ============================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Predict probabilities on validation sequences\n",
        "probs = model_gru.predict(X_val2, batch_size=512, verbose=0).reshape(-1)\n",
        "ytrue = y_val2.astype(int)\n",
        "\n",
        "# 2) Align validation metadata 1:1 with X_val2\n",
        "val_meta2 = meta2.loc[val_mask].copy().reset_index(drop=True)\n",
        "\n",
        "if len(val_meta2) != len(probs):\n",
        "    raise ValueError(f\"Meta/Pred mismatch.  val_meta2={len(val_meta2)} probs={len(probs)}\")\n",
        "\n",
        "val_meta2[\"p_closed\"] = probs\n",
        "val_meta2[\"y_true\"] = ytrue\n",
        "\n",
        "total_pos = int((val_meta2[\"y_true\"] == 1).sum())\n",
        "n = len(val_meta2)\n",
        "\n",
        "print(\"Val windows:\", n)\n",
        "print(\"Val positives:\", total_pos, \"(\", total_pos / max(1, n), \")\")\n",
        "\n",
        "# 3) Top-K% triage: if we only investigate the riskiest K% of windows, what's precision/recall?\n",
        "print(\"\\nTop-K% triage (higher p_closed = higher risk):\")\n",
        "for pct in [0.5, 1, 2, 5, 10]:\n",
        "    k = max(1, int(n * (pct / 100.0)))\n",
        "    topk = val_meta2.sort_values(\"p_closed\", ascending=False).head(k)\n",
        "\n",
        "    tp = int((topk[\"y_true\"] == 1).sum())\n",
        "    precision = tp / max(1, k)\n",
        "    recall = tp / max(1, total_pos)\n",
        "\n",
        "    print(f\"Top {pct:>4}% (k={k:>5}): precision={precision:.3f}  recall={recall:.3f}  tp={tp}\")\n",
        "\n",
        "# 4) Sanity check lists\n",
        "cols = [c for c in [\"business_id\", \"status\", \"start_month\", \"end_month\", \"closure_month\", \"y_true\", \"p_closed\"] if c in val_meta2.columns]\n",
        "\n",
        "print(\"\\nTop 20 highest-risk windows (sanity check):\")\n",
        "print(val_meta2.sort_values(\"p_closed\", ascending=False)[cols].head(20))\n",
        "\n",
        "print(\"\\nBottom 20 lowest-risk windows (sanity check):\")\n",
        "print(val_meta2.sort_values(\"p_closed\", ascending=True)[cols].head(20))"
      ],
      "metadata": {
        "id": "ZYtB3cNv4SGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Cell 20 - BUSINESS-level triage on the validation set (one score per business)\n",
        "# Run AFTER Cell 19 (val_meta2 exists with: business_id, end_month, p_closed, y_true, status)\n",
        "# Output: biz_val_scores + Top-K business triage metrics\n",
        "# ============================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Safety: make sure we have what we need\n",
        "need_cols = [\"business_id\", \"end_month\", \"p_closed\", \"y_true\"]\n",
        "missing = [c for c in need_cols if c not in val_meta2.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"val_meta2 missing required columns: {missing}\")\n",
        "\n",
        "# Ensure datetime sorting\n",
        "val_meta2 = val_meta2.copy()\n",
        "val_meta2[\"end_month\"] = pd.to_datetime(val_meta2[\"end_month\"])\n",
        "\n",
        "# One row per business.  We compute:\n",
        "# - p_last: score on most recent window (this is \"risk now\")\n",
        "# - p_max: worst-case window score (good for \"ever looked bad\")\n",
        "# - p_mean: average window score (stability)\n",
        "# - y_business: business is positive if ANY window is positive in val (since y_true is per-window)\n",
        "biz_val_scores = (\n",
        "    val_meta2.sort_values([\"business_id\", \"end_month\"])\n",
        "      .groupby(\"business_id\", as_index=False)\n",
        "      .agg(\n",
        "          status=(\"status\", \"first\") if \"status\" in val_meta2.columns else (\"business_id\", \"size\"),\n",
        "          end_month_last=(\"end_month\", \"max\"),\n",
        "          p_last=(\"p_closed\", \"last\"),\n",
        "          p_max=(\"p_closed\", \"max\"),\n",
        "          p_mean=(\"p_closed\", \"mean\"),\n",
        "          y_business=(\"y_true\", \"max\"),\n",
        "          n_windows=(\"p_closed\", \"size\"),\n",
        "      )\n",
        ")\n",
        "\n",
        "total_pos_biz = int((biz_val_scores[\"y_business\"] == 1).sum())\n",
        "n_biz = len(biz_val_scores)\n",
        "\n",
        "print(\"Val businesses:\", n_biz)\n",
        "print(\"Val positive businesses (has >=1 positive window):\", total_pos_biz, \"(\", total_pos_biz / max(1, n_biz), \")\")\n",
        "\n",
        "# Choose the score you want to operate on:\n",
        "# - p_last is the right default for \"risk now\"\n",
        "SCORE_COL = \"p_last\"\n",
        "\n",
        "print(f\"\\nBusiness-level Top-K% triage using {SCORE_COL}:\")\n",
        "for pct in [0.5, 1, 2, 5, 10]:\n",
        "    k = max(1, int(n_biz * (pct / 100.0)))\n",
        "    topk = biz_val_scores.sort_values(SCORE_COL, ascending=False).head(k)\n",
        "\n",
        "    tp = int((topk[\"y_business\"] == 1).sum())\n",
        "    precision = tp / max(1, k)\n",
        "    recall = tp / max(1, total_pos_biz)\n",
        "\n",
        "    print(f\"Top {pct:>4}% (k={k:>5}): precision={precision:.3f}  recall={recall:.3f}  tp={tp}\")\n",
        "\n",
        "print(\"\\nTop 25 highest-risk businesses (risk now):\")\n",
        "cols = [c for c in [\"business_id\", \"status\", \"end_month_last\", \"p_last\", \"p_max\", \"p_mean\", \"y_business\", \"n_windows\"] if c in biz_val_scores.columns]\n",
        "print(biz_val_scores.sort_values(SCORE_COL, ascending=False)[cols].head(25))\n",
        "\n",
        "print(\"\\nBottom 25 lowest-risk businesses (risk now):\")\n",
        "print(biz_val_scores.sort_values(SCORE_COL, ascending=True)[cols].head(25))"
      ],
      "metadata": {
        "id": "qGma7XN_5iBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Cell 21 - Add \"recent max\" score (stability / recent-risk safety net)\n",
        "# Goal: catch businesses that spiked recently even if p_last is low.\n",
        "# Requires: val_meta2 (window-level) and biz_val_scores from Cell 20\n",
        "# ============================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# knobs\n",
        "RECENT_WINDOWS = 6  # last 6 windows (roughly last ~6 months of \"ending months\")\n",
        "\n",
        "# Ensure time sort\n",
        "vm = val_meta2.copy()\n",
        "vm[\"end_month\"] = pd.to_datetime(vm[\"end_month\"])\n",
        "\n",
        "# For each business, take last RECENT_WINDOWS windows by end_month, compute max prob\n",
        "recent_max = (\n",
        "    vm.sort_values([\"business_id\", \"end_month\"])\n",
        "      .groupby(\"business_id\", as_index=False)\n",
        "      .tail(RECENT_WINDOWS)\n",
        "      .groupby(\"business_id\", as_index=False)[\"p_closed\"]\n",
        "      .max()\n",
        "      .rename(columns={\"p_closed\": \"p_recent_max\"})\n",
        ")\n",
        "\n",
        "biz_val_scores2 = biz_val_scores.merge(recent_max, on=\"business_id\", how=\"left\")\n",
        "biz_val_scores2[\"p_recent_max\"] = biz_val_scores2[\"p_recent_max\"].fillna(biz_val_scores2[\"p_last\"])\n",
        "\n",
        "# Optional: composite score (weighted toward \"now\", but respects recent spikes)\n",
        "# You can adjust weights.  This is a sane default.\n",
        "biz_val_scores2[\"p_combo\"] = 0.7 * biz_val_scores2[\"p_last\"] + 0.3 * biz_val_scores2[\"p_recent_max\"]\n",
        "\n",
        "print(\"Added columns: p_recent_max, p_combo\")\n",
        "print(biz_val_scores2[[\"business_id\",\"p_last\",\"p_recent_max\",\"p_combo\",\"y_business\"]].head(10))\n",
        "\n",
        "def topk_report(df, score_col, pcts=(0.5,1,2,5,10)):\n",
        "    total_pos = int((df[\"y_business\"] == 1).sum())\n",
        "    n = len(df)\n",
        "    print(f\"\\nTop-K% triage using {score_col}: (n={n}, pos={total_pos}, pos_rate={total_pos/max(1,n):.3f})\")\n",
        "    for pct in pcts:\n",
        "        k = max(1, int(n * (pct / 100.0)))\n",
        "        topk = df.sort_values(score_col, ascending=False).head(k)\n",
        "        tp = int((topk[\"y_business\"] == 1).sum())\n",
        "        precision = tp / max(1, k)\n",
        "        recall = tp / max(1, total_pos)\n",
        "        print(f\"Top {pct:>4}% (k={k:>5}): precision={precision:.3f}  recall={recall:.3f}  tp={tp}\")\n",
        "\n",
        "topk_report(biz_val_scores2, \"p_last\")\n",
        "topk_report(biz_val_scores2, \"p_recent_max\")\n",
        "topk_report(biz_val_scores2, \"p_combo\")\n",
        "\n",
        "print(\"\\nTop 25 by p_combo:\")\n",
        "cols = [c for c in [\"business_id\",\"status\",\"end_month_last\",\"p_last\",\"p_recent_max\",\"p_combo\",\"p_max\",\"p_mean\",\"y_business\",\"n_windows\"] if c in biz_val_scores2.columns]\n",
        "print(biz_val_scores2.sort_values(\"p_combo\", ascending=False)[cols].head(25))\n",
        "\n",
        "print(\"\\nBottom 25 by p_combo:\")\n",
        "print(biz_val_scores2.sort_values(\"p_combo\", ascending=True)[cols].head(25))"
      ],
      "metadata": {
        "id": "PrnNw8En5yCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Cell 22 - Final triage table (business-level) + export\n",
        "# Uses p_recent_max as the primary risk score.\n",
        "# Requires: biz_val_scores2 (from Cell 21)\n",
        "# ============================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "triage = biz_val_scores2.copy()\n",
        "\n",
        "# Primary score\n",
        "triage[\"risk_score\"] = triage[\"p_recent_max\"]\n",
        "\n",
        "# Helpful derived fields\n",
        "triage[\"risk_bucket\"] = pd.cut(\n",
        "    triage[\"risk_score\"],\n",
        "    bins=[-1, 0.2, 0.4, 0.6, 0.8, 1.01],\n",
        "    labels=[\"very_low\", \"low\", \"medium\", \"high\", \"very_high\"]\n",
        ")\n",
        "\n",
        "# Sort for action\n",
        "triage = triage.sort_values([\"risk_score\", \"p_last\"], ascending=False)\n",
        "\n",
        "cols = [c for c in [\n",
        "    \"business_id\",\n",
        "    \"status\",\n",
        "    \"end_month_last\",\n",
        "    \"risk_score\",\n",
        "    \"p_recent_max\",\n",
        "    \"p_last\",\n",
        "    \"p_max\",\n",
        "    \"p_mean\",\n",
        "    \"n_windows\",\n",
        "    \"risk_bucket\",\n",
        "    \"y_business\",\n",
        "] if c in triage.columns]\n",
        "\n",
        "triage_out = triage[cols].reset_index(drop=True)\n",
        "\n",
        "print(\"Triage rows:\", len(triage_out))\n",
        "print(triage_out.head(30))\n",
        "\n",
        "# Export (optional but recommended)\n",
        "OUT_PATH = \"../artifacts/gru_business_triage.csv\"\n",
        "triage_out.to_csv(OUT_PATH, index=False)\n",
        "print(\"Saved:\", OUT_PATH)"
      ],
      "metadata": {
        "id": "9jrmHblV592T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Cell 23 - Workload cutlines (business triage) + export top lists\n",
        "# Run right after Cell 22 (needs triage_out from Cell 22)\n",
        "# ============================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "df = triage_out.copy()\n",
        "\n",
        "score_col = \"risk_score\"   # this is p_recent_max\n",
        "y_col = \"y_business\"\n",
        "\n",
        "n = len(df)\n",
        "pos = int((df[y_col] == 1).sum())\n",
        "pos_rate = pos / max(1, n)\n",
        "\n",
        "print(f\"Businesses: {n}  Positives: {pos}  Pos rate: {pos_rate:.3f}\")\n",
        "print(\"\\nTop-K% workload metrics (sorted by risk_score desc):\")\n",
        "\n",
        "df = df.sort_values(score_col, ascending=False).reset_index(drop=True)\n",
        "\n",
        "for pct in [0.5, 1, 2, 5, 10, 15, 20]:\n",
        "    k = max(1, int(round(n * (pct / 100.0))))\n",
        "    topk = df.head(k)\n",
        "\n",
        "    tp = int((topk[y_col] == 1).sum())\n",
        "    precision = tp / max(1, k)\n",
        "    recall = tp / max(1, pos)\n",
        "\n",
        "    # threshold at this cut (lowest score inside topk)\n",
        "    thr = float(topk[score_col].min())\n",
        "\n",
        "    print(f\"Top {pct:>4}% (k={k:>4})  thr>={thr:.4f}  precision={precision:.3f}  recall={recall:.3f}  tp={tp}\")\n",
        "\n",
        "# Export a couple ready-to-use lists\n",
        "for pct in [5, 10]:\n",
        "    k = max(1, int(round(n * (pct / 100.0))))\n",
        "    out = df.head(k).copy()\n",
        "    out_path = f\"../artifacts/gru_business_triage_top{pct}pct.csv\"\n",
        "    out.to_csv(out_path, index=False)\n",
        "    print(\"Saved:\", out_path)"
      ],
      "metadata": {
        "id": "mgtiyiAa6HWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# ONE CELL - Score ALL businesses, aggregate to business-level triage, save CSVs\n",
        "# Run AFTER your Cell 17 training (model_gru exists) AND after Cell 16 (X2, y2, meta2, train_mask/val_mask exist)\n",
        "# Outputs:\n",
        "#   ../artifacts/gru_business_triage_all.csv\n",
        "#   ../artifacts/gru_business_triage_all_top5pct.csv\n",
        "#   ../artifacts/gru_business_triage_all_top10pct.csv\n",
        "# ============================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---- required objects ----\n",
        "required = [\"model_gru\", \"X2\", \"y2\", \"meta2\"]\n",
        "missing = [r for r in required if r not in globals()]\n",
        "if missing:\n",
        "    raise RuntimeError(f\"Missing required objects in memory: {missing}.  \"\n",
        "                       f\"Run your sequence build (Cell 15/16) and GRU training (Cell 17) first.\")\n",
        "\n",
        "# ---- ensure artifacts dir ----\n",
        "OUT_DIR = \"../artifacts\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---- score all windows ----\n",
        "probs_all = model_gru.predict(X2, batch_size=512, verbose=0).reshape(-1).astype(float)\n",
        "\n",
        "tri = meta2.copy().reset_index(drop=True)\n",
        "tri[\"p_closed\"] = probs_all\n",
        "tri[\"y_true_window\"] = np.asarray(y2, dtype=int)\n",
        "\n",
        "# ---- helper: recent window filter (last N months of windows per business, based on end_month) ----\n",
        "RECENT_MONTHS = 12  # define \"recent\" windows as those ending within the last 12 months of that business's observed timeline\n",
        "\n",
        "def add_business_level_features(g: pd.DataFrame) -> pd.Series:\n",
        "    g = g.sort_values(\"end_month\")\n",
        "\n",
        "    p_last = float(g[\"p_closed\"].iloc[-1])\n",
        "    p_max  = float(g[\"p_closed\"].max())\n",
        "    p_mean = float(g[\"p_closed\"].mean())\n",
        "    n_windows = int(len(g))\n",
        "\n",
        "    # define recent cutoff relative to that business's last observed end_month\n",
        "    end_last = pd.to_datetime(g[\"end_month\"].iloc[-1])\n",
        "    recent_cut = end_last - pd.DateOffset(months=RECENT_MONTHS - 1)\n",
        "    g_recent = g[pd.to_datetime(g[\"end_month\"]) >= recent_cut]\n",
        "\n",
        "    if len(g_recent) == 0:\n",
        "        p_recent_max = p_last\n",
        "    else:\n",
        "        p_recent_max = float(g_recent[\"p_closed\"].max())\n",
        "\n",
        "    # combo score: emphasize recent spike more than stale history\n",
        "    # (this is what improved your Top-K metrics)\n",
        "    p_combo = (0.65 * p_recent_max) + (0.35 * p_last)\n",
        "\n",
        "    # business truth label in THIS dataset:\n",
        "    # y_business = 1 if business has >=1 positive (y=1) window, else 0\n",
        "    y_business = int((g[\"y_true_window\"] == 1).any())\n",
        "\n",
        "    return pd.Series({\n",
        "        \"status\": g[\"status\"].iloc[0],\n",
        "        \"end_month_last\": end_last,\n",
        "        \"p_last\": p_last,\n",
        "        \"p_recent_max\": p_recent_max,\n",
        "        \"p_max\": p_max,\n",
        "        \"p_mean\": p_mean,\n",
        "        \"p_combo\": float(p_combo),\n",
        "        \"n_windows\": n_windows,\n",
        "        \"y_business\": y_business\n",
        "    })\n",
        "\n",
        "biz_triage = (\n",
        "    tri.groupby(\"business_id\", as_index=False)\n",
        "       .apply(add_business_level_features)\n",
        "       .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# ---- choose your final risk score (best performer in your run) ----\n",
        "biz_triage[\"risk_score\"] = biz_triage[\"p_recent_max\"]\n",
        "\n",
        "# ---- bucketize risk for dashboard ----\n",
        "# Use quantiles so buckets are stable-ish across runs\n",
        "q = biz_triage[\"risk_score\"].quantile([0.80, 0.90, 0.95]).to_dict()\n",
        "q80, q90, q95 = float(q[0.80]), float(q[0.90]), float(q[0.95])\n",
        "\n",
        "def bucket(v: float) -> str:\n",
        "    if v >= q95: return \"very_high\"\n",
        "    if v >= q90: return \"high\"\n",
        "    if v >= q80: return \"medium\"\n",
        "    return \"low\"\n",
        "\n",
        "biz_triage[\"risk_bucket\"] = biz_triage[\"risk_score\"].map(bucket)\n",
        "\n",
        "# ---- sort + save full triage ----\n",
        "biz_triage = biz_triage.sort_values(\"risk_score\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "all_path = os.path.join(OUT_DIR, \"gru_business_triage_all.csv\")\n",
        "biz_triage.to_csv(all_path, index=False)\n",
        "print(\"Saved:\", all_path)\n",
        "\n",
        "# ---- workload metrics + top lists (by percent) ----\n",
        "n = len(biz_triage)\n",
        "pos = int((biz_triage[\"y_business\"] == 1).sum())\n",
        "pos_rate = pos / max(1, n)\n",
        "print(\"\\nBusinesses:\", n, \" Positives:\", pos, \" Pos rate:\", pos_rate)\n",
        "\n",
        "print(\"\\nTop-K% workload metrics (sorted by risk_score desc):\")\n",
        "for pct in [0.5, 1, 2, 5, 10, 15, 20]:\n",
        "    k = max(1, int(round(n * (pct / 100.0))))\n",
        "    topk = biz_triage.head(k)\n",
        "    tp = int((topk[\"y_business\"] == 1).sum())\n",
        "    precision = tp / max(1, k)\n",
        "    recall = tp / max(1, pos)\n",
        "\n",
        "    thr = float(topk[\"risk_score\"].iloc[-1])\n",
        "    print(f\"Top {pct:>4}% (k={k:>4})  thr>={thr:.4f}  precision={precision:.3f}  recall={recall:.3f}  tp={tp}\")\n",
        "\n",
        "# ---- save top 5% and top 10% convenience files ----\n",
        "k5 = max(1, int(round(n * 0.05)))\n",
        "k10 = max(1, int(round(n * 0.10)))\n",
        "\n",
        "top5 = biz_triage.head(k5).copy()\n",
        "top10 = biz_triage.head(k10).copy()\n",
        "\n",
        "top5_path = os.path.join(OUT_DIR, \"gru_business_triage_all_top5pct.csv\")\n",
        "top10_path = os.path.join(OUT_DIR, \"gru_business_triage_all_top10pct.csv\")\n",
        "\n",
        "top5.to_csv(top5_path, index=False)\n",
        "top10.to_csv(top10_path, index=False)\n",
        "\n",
        "print(\"Saved:\", top5_path)\n",
        "print(\"Saved:\", top10_path)\n",
        "\n",
        "# ---- show head ----\n",
        "display_cols = [\"business_id\",\"status\",\"end_month_last\",\"risk_score\",\"p_recent_max\",\"p_last\",\"p_max\",\"p_mean\",\"n_windows\",\"risk_bucket\",\"y_business\"]\n",
        "print(\"\\nTop 30 triage rows:\")\n",
        "print(biz_triage[display_cols].head(30))"
      ],
      "metadata": {
        "id": "OXXPBqVUfZQ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}